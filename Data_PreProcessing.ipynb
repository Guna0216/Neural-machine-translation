{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1Yw1VPhxOp80"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3g9C349u7C6R",
    "outputId": "286753a4-be08-42f1-863c-3fed24a84dce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/shri/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aVdn1vh7Op82"
   },
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(device='mps')\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(device='cuda')\n",
    "else:\n",
    "    DEVICE = torch.device(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DQlMDGcmqqAQ"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train_full.csv')\n",
    "test_df = pd.read_csv('data/test_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jVgnoYiUxCZC"
   },
   "outputs": [],
   "source": [
    "train_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8JbvsMDOp83"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UEPaL1e0wW4P"
   },
   "outputs": [],
   "source": [
    "def preprocessing_text(x: str) -> str:\n",
    "  \"\"\"Preprocess the text\n",
    "\n",
    "  Args:\n",
    "      x (str): Input String\n",
    "\n",
    "  Returns:\n",
    "      str: Processed String\n",
    "  \"\"\"\n",
    "  x = x.lower().strip()\n",
    "  x = re.sub(r\"([.!?])\", r\" \\1\", x)\n",
    "  x = re.sub(r\"[^a-zA-Z!?]+\", r\" \", x)\n",
    "  return x.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OTQhdqaowXnq"
   },
   "outputs": [],
   "source": [
    "train_df['English'] = train_df['English'].apply(preprocessing_text)\n",
    "train_df['French'] = train_df['French'].apply(preprocessing_text)\n",
    "\n",
    "test_df['English'] = test_df['English'].apply(preprocessing_text)\n",
    "test_df['French'] = test_df['French'].apply(preprocessing_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NlyEQd5WxPJu"
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 25\n",
    "\n",
    "def get_max_seq_length_data(data_frame: pd.DataFrame) -> pd.DataFrame:\n",
    "  \"\"\"Get Sentences of max sequence lenght\n",
    "\n",
    "  Args:\n",
    "      data_frame (pd.DataFrame): Input Data Frame will all data\n",
    "\n",
    "  Returns:\n",
    "      pd.DataFrame: Data Frame with data of max sequnce lenghth.\n",
    "  \"\"\"\n",
    "  english = []\n",
    "  french = []\n",
    "  for row in data_frame.iterrows():\n",
    "    if len(nltk.word_tokenize(row[1]['English'])) <= MAX_SEQ_LENGTH and len(nltk.word_tokenize(row[1]['English'])) >= 1 and len(nltk.word_tokenize(row[1]['French'])) <= MAX_SEQ_LENGTH and len(nltk.word_tokenize(row[1]['French'])) >= 1:\n",
    "      english.append(row[1]['English'])\n",
    "      french.append(row[1]['French'])\n",
    "  return pd.DataFrame({'English': english, 'French': french})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Hq8C1mVWxO8q"
   },
   "outputs": [],
   "source": [
    "train_df = get_max_seq_length_data(train_df)\n",
    "test_df = get_max_seq_length_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "50wuIDCSRk_C"
   },
   "outputs": [],
   "source": [
    "class Language:\n",
    "  \"\"\"Class with language that maintains all the information of the language corpus\n",
    "  \"\"\"\n",
    "  def __init__(self, language_name: str) -> None:\n",
    "    \"\"\"Initializing the language\n",
    "\n",
    "    Args:\n",
    "        language_name (str): language name\n",
    "    \"\"\"\n",
    "    self.language_name = language_name\n",
    "    self.word2index = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "    self.word2count = {}\n",
    "    self.index2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "    self.n_words = 4\n",
    "\n",
    "  def addSentence(self, sentence: str) -> None:\n",
    "    \"\"\"Process the sentence\n",
    "\n",
    "    Args:\n",
    "        sentence (str): Strings that we want to add to corpus\n",
    "    \"\"\"\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "      self.addWord(word)\n",
    "\n",
    "  def addWord(self, word: str) -> None:\n",
    "    \"\"\"Add the word to corpus.\n",
    "\n",
    "    Args:\n",
    "        word (str): Input word that we want to add to corpus.\n",
    "    \"\"\"\n",
    "    if word not in self.word2index:\n",
    "      self.word2index[word] = self.n_words\n",
    "      self.word2count[word] = 1\n",
    "      self.index2word[self.n_words] = word\n",
    "      self.n_words += 1\n",
    "\n",
    "    else:\n",
    "      self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "I_hSIKzURk7-"
   },
   "outputs": [],
   "source": [
    "english = Language('english')\n",
    "french = Language('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "rWFYpOjyrNFD"
   },
   "outputs": [],
   "source": [
    "for row in train_df.iterrows():\n",
    "  english.addSentence(row[1]['English'])\n",
    "  french.addSentence(row[1]['French'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MgARpQmE0FFe"
   },
   "outputs": [],
   "source": [
    "def create_data_from_vocab(sentence: str, language_corpus: 'Language') -> list[int]:\n",
    "  \"\"\"Create dataset from vocabulary\n",
    "\n",
    "  Args:\n",
    "      sentence (str): Input sentence that needs to be processed.\n",
    "      language_corpus (Language): Reference language corpus\n",
    "\n",
    "  Returns:\n",
    "      list[int]: Sentence vector\n",
    "  \"\"\"\n",
    "  sentence_vector = []\n",
    "  for word in nltk.word_tokenize(sentence):\n",
    "    if word not in language_corpus.word2index:\n",
    "      sentence_vector.append(1)\n",
    "    else:\n",
    "      sentence_vector.append(language_corpus.word2index[word])\n",
    "  sentence_vector.append(language_corpus.word2index['<EOS>'])\n",
    "\n",
    "  return sentence_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ZWrlkOvY1-fr"
   },
   "outputs": [],
   "source": [
    "train_df['English'] = train_df['English'].apply((lambda x: create_data_from_vocab(x, english)))\n",
    "train_df['French'] = train_df['French'].apply((lambda x: create_data_from_vocab(x, french)))\n",
    "\n",
    "test_df['English'] = test_df['English'].apply((lambda x: create_data_from_vocab(x, english)))\n",
    "test_df['French'] = test_df['French'].apply((lambda x: create_data_from_vocab(x, french)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "DYcJV_R83Koj"
   },
   "outputs": [],
   "source": [
    "def create_sentence_vectors(data_frame: pd.DataFrame) -> list:\n",
    "  \"\"\"_summary_\n",
    "\n",
    "  Args:\n",
    "      data_frame (pd.DataFrame): create dataset\n",
    "  Returns:\n",
    "      list: Dataset\n",
    "  \"\"\"\n",
    "  x = []\n",
    "  y = []\n",
    "  for row in data_frame.iterrows():\n",
    "    x.append(row[1]['English'])\n",
    "    y.append(row[1]['French'])\n",
    "\n",
    "  sorted_lists = sorted(zip(x,y), key=lambda x:len(x[0]))\n",
    "  return zip(*sorted_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OZqRQrwI3oFb"
   },
   "outputs": [],
   "source": [
    "train_x, train_y = create_sentence_vectors(train_df)\n",
    "test_x, test_y = create_sentence_vectors(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZDfuQ694MLV"
   },
   "source": [
    "#### Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ZqcCd3D5dq1M"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, src_text: str, tgt_text: str):\n",
    "        \"\"\"Initiate Custom dataset.\n",
    "\n",
    "        Args:\n",
    "            src_text (str): input text\n",
    "            tgt_text (str): target text\n",
    "        \"\"\"\n",
    "        self.src_text = src_text\n",
    "        self.tgt_text = tgt_text\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple:\n",
    "        \"\"\"get an item\n",
    "\n",
    "        Args:\n",
    "            index (int): index number\n",
    "\n",
    "        Returns:\n",
    "            tuple: tuple of src text and target text\n",
    "        \"\"\"\n",
    "        return self.src_text[index], self.tgt_text[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: length of the data set\n",
    "        \"\"\"\n",
    "        return len(self.src_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ekUZR1Zy4KH_"
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_x, train_y)\n",
    "test_dataset = CustomDataset(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "A2zvwcrQeHLd"
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # Separate the source and target texts\n",
    "    src_texts, tgt_texts = zip(*batch)\n",
    "\n",
    "    # Step 1: Determine maximum sequence length\n",
    "    max_length = max(max(len(seq) for seq in src_texts), max(len(seq) for seq in tgt_texts))\n",
    "\n",
    "    # Step 2: Convert to tensors\n",
    "    src_tensors = [torch.tensor(seq) for seq in src_texts]\n",
    "    tgt_tensors = [torch.tensor(seq) for seq in tgt_texts]\n",
    "\n",
    "    # Step 3: Pad sequences for source and target tensors\n",
    "    padded_src_sequences = torch.nn.utils.rnn.pad_sequence(src_tensors, batch_first=True, padding_value=english.word2index['<PAD>'])\n",
    "    padded_tgt_sequences = torch.nn.utils.rnn.pad_sequence(tgt_tensors, batch_first=True, padding_value=french.word2index['<PAD>'])\n",
    "\n",
    "    return padded_src_sequences, padded_tgt_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kNwcLtt33ARg"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, collate_fn=custom_collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size, collate_fn=custom_collate_fn)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
